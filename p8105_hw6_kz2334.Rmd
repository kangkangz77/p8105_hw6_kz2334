---
title: "p8105_hw6_kz2334"
author: "Kangkang Zhang"
output: github_document
---

```{r set up, include = FALSE}
library(tidyverse)
library(purrr)
library(mgcv)
library(modelr)

set.seed(1)

theme_set(theme_bw() + theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5)))
```


#Problem 1

Import the data. 
```{r 1.1 import, message = FALSE} 
homicide_data = read_csv("./data/homicide-data.csv")
```

Create a city_state variable, and a binary variable indicating whether the homicide is solved. 

```{r 1.2}
homicide_data = homicide_data %>% 
  mutate(city_state = str_c(city, ", ", state),
         status = ifelse(disposition %in% c("Closed without arrest", "Open/No arrest"), 0, 1))
```

Omit cities Dallas, TX; Phoenix, AZ; Kansas City, MO and Tulsa, AL. Modifiy victim_race to have categories white and non-white, with white as the reference category. Convert victim_age to numeric.

```{r 1.3, warning = FALSE}
homicide_md = homicide_data %>% 
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))) %>% 
  mutate(victim_race = ifelse(victim_race == "White", "white", "non-white"),
         victim_race = as.factor(victim_race), 
         victim_race = relevel(victim_race, ref = "white"),
          victim_age = as.numeric(victim_age))
  
```

---

Run logistic regression for the city of Baltimore, MD. Find out the CI and estimate of the adjusted odds ratio for solving homicides comparing non-white victims to white victims keeping all other variables fixed.

```{r 1.4, message = FALSE}
balt_logistic = homicide_md %>% 
  filter(city_state == "Baltimore, MD") %>% 
  glm(status ~ victim_age + victim_sex + victim_race, data = ., family = binomial())

balt_logistic %>% 
  broom::tidy() %>%
  #extract OR and CIs for OR
  mutate(OR = exp(estimate),
    conf_low_or = exp(confint(balt_logistic, level = 0.95)[,1]),
    conf_high_or = exp(confint(balt_logistic, level = 0.95)[,2])) %>% 
  filter(term == "victim_racenon-white") %>% 
  select(., OR, starts_with("conf")) %>% 
  knitr::kable(digit = 3)
```

We can see that adjusted OR for solving homicides comparing non-white victims to white victims in Baltimore is 0.441. 95% CI is (0.312, 0.620). The OR solving homicides for non-white victims is 0.441 times of that for white victims, which means non-white victims are less likely to be with solving homicides.

---

Run glm for each of the cities, and extract the adjusted odds ratio (and CI) for solving homicides comparing non-white victims to white victims. Create a dataframe with estimated ORs and CIs for each city.

```{r 1.5, message = FALSE, warning = FALSE}
homicide_city = homicide_md %>% 
  group_by(city_state) %>% 
  nest() %>%
  #run glm, extract estimates and CIs
  mutate(logistic = map(data, ~glm(status ~ victim_age + victim_sex + victim_race, 
                                   data = .x, family = binomial())),
         result = map(logistic, broom::tidy),
         conf = map(logistic, confint), 
         result_comb = map2(result, conf, cbind)) %>%
  select(city_state, result_comb) %>% 
  unnest() %>%
  filter(term == "victim_racenon-white") %>%
  janitor::clean_names() %>%
  #calculate OR and CIs
  mutate(OR = exp(estimate),
    conf_low_or = exp(x2_5_percent),
    conf_high_or = exp(x97_5_percent)) %>% 
  select(city_state, OR, starts_with("conf")) 

homicide_city %>% 
  knitr::kable(digit = 3)
```

---

Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR.

```{r 1.6, fig.align= 'center'}
homicide_city %>%
  mutate( city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf_low_or, ymax = conf_high_or), width = 0.2) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(
    title = "OR and CI for solving homicides comparing non-white victims to white victims",
    x = "City",
    y = "Estimated OR and CI"
  ) 
```

According to the plot, most of the cities have ORs less than 1, which means in those cities, non-white victims are less likely to be with solving homicides. Boston has the smallest value of OR, closing to zero. Tampa has the largest value of OR, slightly larger than 1.

#Problem 2

###Load and clean the data

```{r 2.1, message = FALSE}
birth_data = read_csv("./data/birthweight.csv")

birth_data = birth_data %>%
  #convert categorical variables to factor
  mutate( babysex = as.factor(babysex),
          frace = as.factor(frace),
          malform = as.factor(malform),
          mrace = as.factor(mrace)
          )

#check NA values
birth_data %>% 
  summarise_all(funs(sum(is.na(.)))) %>% 
  knitr::kable()

#check zero-equal values
birth_data %>% 
  summarise_all(funs(sum(. == 0))) %>% 
  knitr::kable()

#omit observations have missing values
birth_data = birth_data%>% 
  filter(menarche != 0 & fincome != 0)
```

Follow the dictionary of the dataset, I convert 4 variables to factor. Then I find out there is no  NA value in each variable. Then I check if there are zero values. I find that there is one missing value in family income and one missing value in motherâ€™s age at menarche. Luckily this is a small number.

###build my own model

---

Check whether the variable birth weight follow normal distribution.

```{r}
birth_data %>% 
  ggplot(aes(x = bwt)) +
  geom_density() +
  labs(
    title = "Histogram of birth weight",
    x = "bwt"
  )
```

The distribution of birth weight is well bell-shaped, we can assume that the variable follows normal distribution.

---

Check corrolations between variables.

```{r 2.2, warning = FALSE}
#check corrolation
birth_data %>% 
  select_if(is.numeric) %>%
  select(bwt, everything()) %>% 
  cor() %>% 
  knitr::kable(digits = 3)
```

We can find that variables pnumsga and pnumlbw have NA value of corrolation of any other variables because the values of them equals to zero in all observartions. So we can exclude them from the model. The corrolation of responce and parity is  -0.008, the closet to zero. So I decide to exclude it.

---

Use VIF to select variables untill all preditors have VIF less than 10.

```{r 2.3}
#delete varibles mentioned above
birth_select1 = birth_data %>% 
  select_if(is.numeric) %>%
  select(bwt, everything(), - pnumsga, - pnumlbw, - parity) 

#use vif to select variables
model1 = lm(bwt ~., birth_select1)  
model1

#delete NA variable wtgain

birth_select2 = birth_select1 %>% 
  select(-wtgain)

model2 = lm(bwt ~., birth_select2)  
HH::vif(model2) 

#delete ppwt with largest vif

birth_select3 = birth_select2 %>% 
  select(-ppwt)

model3 = lm(bwt ~., birth_select3)  
HH::vif(model3) 
```

Now we can see that all variables left in the model have vif less than 10, we conclude that there is no multicollinearity among them.

---

Then we use AIC criterion for var selection then keep variables with p-value < 0.05.

```{r 2.4, message = FALSE}
birth_select = birth_data %>%
  select(- pnumsga, - pnumlbw, - parity, - wtgain, - ppwt) 

#use backward method for AIC criterion
reg1 = lm(bwt ~., birth_select) 
reg2 = step(reg1, direction='backward')  
summary(reg2)

#remove mheight since its p-value is the largest and > 0.05
reg3 = update(reg2, . ~ . - mheight) 
summary(reg3)

#remove fincome since its p-value is the largest and > 0.05
reg4 = update(reg3, . ~ . - fincome) 
summary(reg4) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)

summary(reg4) %>% 
  broom::glance() %>% 
  knitr::kable(digits = 3)
```

The factor mrace variable has four categories. Only one category has p-value > 0.05, and close to 0.05. I decide to keep this variable.

Then we have a regression model for birthweight, with 2 factor predictors and 6 numeric predictors.The adjusted $R^2$ is 0.717, which means the model is comparatively good. The F statistics in global test is 1098.414, p-value is 0. The model is significant.

---

Draw a plot of model residuals against fitted values.

```{r 2.5, fig.align= 'center'}
birth_select %>% 
  add_predictions(reg4) %>% 
  add_residuals(reg4) %>% 
  ggplot(aes(x = pred, y = resid)) + geom_point()
```

###Model comparison

Split train and test sets for 100 times cross validation.
```{r}
cv_df = 
  crossv_mc(birth_data, 100) 
```

---

Build each model on every train sets and calculate rmse.

```{r}
cv_df_result = 
  cv_df %>% 
  mutate(mod_my = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + gaweeks + 
                                  mrace + ppbmi + smoken, data = .x)),
         mod_1 = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         mod_2 = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  mutate(rmse_my    = map2_dbl(mod_my, test, ~rmse(model = .x, data = .y)),
         rmse_1 = map2_dbl(mod_1, test, ~rmse(model = .x, data = .y)),
         rmse_2 = map2_dbl(mod_2, test, ~rmse(model = .x, data = .y)))
```

---

Make a violin plot to show the distribution of rmse for each model.

```{r, fig.align= 'center'}
cv_df_result %>% 
  select(starts_with("rmse")) %>% 
  gather(key = model, value = rmse) %>%
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() +
  scale_x_discrete(labels = c("My Model", "Model 1", "Model 2")) + 
  labs(
    title = "Distribution of RMSE for each model",
    x = "Model",
    y = "RMSE"
  ) 
```

Based on e plot, we can see my model has the smallest RMSE among those models. It shows that adding more reasonable variables or interactions into the model can lead to improvements in predictive accuracy. But in the mean times it also results in model complexity. In my opinion I would chooce my model to predict birth weight.




